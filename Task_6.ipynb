{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Данные\n",
    "\n",
    "Данные в [архиве](https://drive.google.com/file/d/15o7fdxTgndoy6K-e7g8g1M2-bOOwqZPl/view?usp=sharing). В нём два файла:\n",
    "- `news_train.txt` тренировочное множество\n",
    "- `news_test.txt` тренировочное множество\n",
    "\n",
    "С некоторых новостных сайтов были загружены тексты новостей за период  несколько лет, причем каждая новость принаделжит к какой-то рубрике: `science`, `style`, `culture`, `life`, `economics`, `business`, `travel`, `forces`, `media`, `sport`.\n",
    "\n",
    "В каждой строке файла содержится метка рубрики, заголовок новостной статьи и сам текст статьи, например:\n",
    "\n",
    ">    **sport**&nbsp;&lt;tab&gt;&nbsp;**Сборная Канады по хоккею разгромила чехов**&nbsp;&lt;tab&gt;&nbsp;**Сборная Канады по хоккею крупно об...**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Задача\n",
    "\n",
    "1. Обработать данные, получив для каждого текста набор токенов\n",
    "Обработать токены с помощью (один вариант из трех):\n",
    "    - pymorphy2\n",
    "    - русского [snowball стеммера](https://www.nltk.org/howto/stem.html)\n",
    "    - [SentencePiece](https://github.com/google/sentencepiece) или [Huggingface Tokenizers](https://github.com/huggingface/tokenizers)\n",
    "    \n",
    "    \n",
    "2. Обучить word embeddings (fastText, word2vec, gloVe) на тренировочных данных. Можно использовать [gensim](https://radimrehurek.com/gensim/models/word2vec.html) . Продемонстрировать семантические ассоциации. \n",
    "\n",
    "3. Реализовать алгоритм классификации, посчитать точноть на тестовых данных, подобрать гиперпараметры. Метод векторизации выбрать произвольно - можно использовать $tf-idf$ с понижением размерности (см. scikit-learn), можно использовать обученные на предыдущем шаге векторные представления, можно использовать [предобученные модели](https://rusvectores.org/ru/models/). Имейте ввиду, что простое \"усреднение\" токенов в тексте скорее всего не даст положительных результатов. Нужно реализовать два алгоритмов из трех:\n",
    "     - SVM\n",
    "     - наивный байесовский классификатор\n",
    "     - логистическая регрессия\n",
    "    \n",
    "\n",
    "4.* Реализуйте классификацию с помощью нейросетевых моделей. Например [RuBERT](http://docs.deeppavlov.ai/en/master/features/models/bert.html) или [ELMo](https://rusvectores.org/ru/models/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "lines = list(open('./news_train.txt', 'r', encoding='utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from pymorphy2.tokenizers import simple_word_tokenize\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Читаем данные"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class</th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sport</td>\n",
       "      <td>Овечкин пожертвовал детской хоккейной школе ав...</td>\n",
       "      <td>Нападающий «Вашингтон Кэпиталз» Александр Овеч...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>culture</td>\n",
       "      <td>Рекордно дорогую статую майя признали подделкой</td>\n",
       "      <td>Власти Мексики объявили подделкой статую майя,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>science</td>\n",
       "      <td>Samsung представила флагман в защищенном корпусе</td>\n",
       "      <td>Южнокорейская Samsung анонсировала защищенную ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>sport</td>\n",
       "      <td>С футболиста «Спартака» сняли четырехматчевую ...</td>\n",
       "      <td>Контрольно-дисциплинарный комитет (КДК) РФС сн...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>media</td>\n",
       "      <td>Hopes &amp; Fears объединится с The Village</td>\n",
       "      <td>Интернет-издание Hopes &amp; Fears объявило о свое...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     class                                              title  \\\n",
       "0    sport  Овечкин пожертвовал детской хоккейной школе ав...   \n",
       "1  culture    Рекордно дорогую статую майя признали подделкой   \n",
       "2  science   Samsung представила флагман в защищенном корпусе   \n",
       "3    sport  С футболиста «Спартака» сняли четырехматчевую ...   \n",
       "4    media            Hopes & Fears объединится с The Village   \n",
       "\n",
       "                                                text  \n",
       "0  Нападающий «Вашингтон Кэпиталз» Александр Овеч...  \n",
       "1  Власти Мексики объявили подделкой статую майя,...  \n",
       "2  Южнокорейская Samsung анонсировала защищенную ...  \n",
       "3  Контрольно-дисциплинарный комитет (КДК) РФС сн...  \n",
       "4  Интернет-издание Hopes & Fears объявило о свое...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train = pd.read_csv('../data/news_train.txt', sep='\\t', names=['class', 'title', 'text'])\n",
    "df_test = pd.read_csv('../data/news_test.txt', sep='\\t', names=['class', 'title', 'text'])\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Токенизация и стемминг"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(line):\n",
    "    tokens = simple_word_tokenize(line)\n",
    "    stems = [\n",
    "        snowball.stem(x.lower())\n",
    "        for x in tokens\n",
    "        if x not in stopwords_ru\n",
    "    ]\n",
    "    return stems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████| 15000/15000 [01:50<00:00, 135.19it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████| 3000/3000 [00:22<00:00, 135.96it/s]\n"
     ]
    }
   ],
   "source": [
    "snowball = SnowballStemmer(language=\"russian\")\n",
    "stopwords_ru = stopwords.words('russian')\n",
    "\n",
    "df_train['text_stems'] = df_train.text.progress_apply(tokenize)\n",
    "df_test['text_stems'] = df_test.text.progress_apply(tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class</th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>text_stems</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sport</td>\n",
       "      <td>Овечкин пожертвовал детской хоккейной школе ав...</td>\n",
       "      <td>Нападающий «Вашингтон Кэпиталз» Александр Овеч...</td>\n",
       "      <td>[напада, «, вашингтон, кэпиталз, », александр,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>culture</td>\n",
       "      <td>Рекордно дорогую статую майя признали подделкой</td>\n",
       "      <td>Власти Мексики объявили подделкой статую майя,...</td>\n",
       "      <td>[власт, мексик, объяв, подделк, стат, май, ,, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>science</td>\n",
       "      <td>Samsung представила флагман в защищенном корпусе</td>\n",
       "      <td>Южнокорейская Samsung анонсировала защищенную ...</td>\n",
       "      <td>[южнокорейск, samsung, анонсирова, защищен, ве...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>sport</td>\n",
       "      <td>С футболиста «Спартака» сняли четырехматчевую ...</td>\n",
       "      <td>Контрольно-дисциплинарный комитет (КДК) РФС сн...</td>\n",
       "      <td>[контрольно-дисциплинарн, комитет, (, кдк, ), ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>media</td>\n",
       "      <td>Hopes &amp; Fears объединится с The Village</td>\n",
       "      <td>Интернет-издание Hopes &amp; Fears объявило о свое...</td>\n",
       "      <td>[интернет-издан, hopes, &amp;, fears, объяв, сво, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     class                                              title  \\\n",
       "0    sport  Овечкин пожертвовал детской хоккейной школе ав...   \n",
       "1  culture    Рекордно дорогую статую майя признали подделкой   \n",
       "2  science   Samsung представила флагман в защищенном корпусе   \n",
       "3    sport  С футболиста «Спартака» сняли четырехматчевую ...   \n",
       "4    media            Hopes & Fears объединится с The Village   \n",
       "\n",
       "                                                text  \\\n",
       "0  Нападающий «Вашингтон Кэпиталз» Александр Овеч...   \n",
       "1  Власти Мексики объявили подделкой статую майя,...   \n",
       "2  Южнокорейская Samsung анонсировала защищенную ...   \n",
       "3  Контрольно-дисциплинарный комитет (КДК) РФС сн...   \n",
       "4  Интернет-издание Hopes & Fears объявило о свое...   \n",
       "\n",
       "                                          text_stems  \n",
       "0  [напада, «, вашингтон, кэпиталз, », александр,...  \n",
       "1  [власт, мексик, объяв, подделк, стат, май, ,, ...  \n",
       "2  [южнокорейск, samsung, анонсирова, защищен, ве...  \n",
       "3  [контрольно-дисциплинарн, комитет, (, кдк, ), ...  \n",
       "4  [интернет-издан, hopes, &, fears, объяв, сво, ...  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec = Word2Vec(df_train.text_stems,\n",
    "                    size=128,\n",
    "                    window=5,\n",
    "                    sg=1,  # skip-gram\n",
    "                    workers=2)\n",
    "\n",
    "vec = word2vec.wv.vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('нью-джерс', 0.7245336771011353),\n",
       " ('кэпиталс', 0.7167192697525024),\n",
       " ('колорад', 0.7156737446784973),\n",
       " ('миннесот', 0.7052792310714722),\n",
       " ('эвеланш', 0.6969218254089355),\n",
       " ('сан-хос', 0.6897116899490356),\n",
       " ('пантерс', 0.677248477935791),\n",
       " ('лос-анджелес', 0.674209475517273),\n",
       " ('рейнджерс', 0.6714686155319214),\n",
       " ('оклахом', 0.6680904626846313)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2vec.most_similar('вашингтон')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('жилищ', 0.9093278646469116),\n",
       " ('побежа', 0.9044625759124756),\n",
       " ('медвежат', 0.9021425247192383),\n",
       " ('копытн', 0.9020285606384277),\n",
       " ('заперт', 0.9017809629440308),\n",
       " ('колодец', 0.8995034694671631),\n",
       " ('крокодил', 0.8992606401443481),\n",
       " ('медведиц', 0.8989448547363281),\n",
       " ('загон', 0.8969249725341797),\n",
       " ('поеда', 0.8950693011283875)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2vec.most_similar('питон')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Классификатор на tf–idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(pipeline, X_train, y_train, X_val, y_val):\n",
    "    y_val_pred = pipeline.predict(X_val)\n",
    "    print(f'{pipeline.steps[-1][1].__class__.__name__} test accuracy: {accuracy_score(y_val, y_val_pred)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer()\n",
    "\n",
    "X_train = tfidf.fit_transform(df_train.text_stems.apply(lambda x: ' '.join(x)))\n",
    "y_train = df_train['class'].values\n",
    "\n",
    "X_test = tfidf.transform(df_test.text_stems.apply(lambda x: ' '.join(x)))\n",
    "y_test = df_test['class'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DummyClassifier test accuracy: 0.12833333333333333\n",
      "LogisticRegression test accuracy: 0.8733333333333333\n",
      "SVC test accuracy: 0.8726666666666667\n"
     ]
    }
   ],
   "source": [
    "models = {\n",
    "    DummyClassifier(): {},\n",
    "    LogisticRegression(class_weight='balanced'): {'model__solver': ['newton-cg', 'lbfgs', 'saga']},\n",
    "    SVC(class_weight='balanced'): {'model__kernel': ['linear', 'rbf']},\n",
    "}\n",
    "\n",
    "for model, params in models.items():\n",
    "    pipeline = Pipeline([\n",
    "        ('feature_selection', SelectKBest()),\n",
    "        ('model', model),\n",
    "    ])\n",
    "\n",
    "    param_grid = {\n",
    "        **params,\n",
    "        **{'feature_selection__k': [6000, 8000]}\n",
    "    }\n",
    "\n",
    "    gs = GridSearchCV(pipeline,\n",
    "                      param_grid=param_grid,\n",
    "                      cv=3,\n",
    "                      refit=True,\n",
    "                      verbose=0)\n",
    "\n",
    "    gs.fit(X_train, y_train)\n",
    "    evaluate(gs.best_estimator_, X_train, y_train, X_test, y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
